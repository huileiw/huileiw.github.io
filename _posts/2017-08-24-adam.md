---
layout: single
title:  "Adam Optimizer - Part I"
date:   2017-08-24
tags: [Optimizer, Machine Learning, Deep Learning]
excerpt: Understanding Adam and RMSProp Optimizer
comments: true
---
A list of various optimizers for machine learning and deep learning are introduced in the [Stanford CS231N Neural Networks for Visual Recognition](http://cs231n.github.io/neural-networks-3/#second) course. [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimizer is recommended as the default algorithm to use, so a sufficient understanding of it will be very helpful. In this post, I delve deeper to understand the formulation of Adam, and take a stab at illustrating the idea behind it and why it works better than other optimizers. In a follow up post, I will use a ConvNet trained on [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset as an example to demonstrate its superiority.  

The name 'Adam' is derived from adaptive moment estimation. There are two elements that we need to understand, namely 'adaptive' and 'moment'. But before we go there, what are we trying to 'estimate'? Here is the background in two sentences. In machine learning, there are many times no closed-form solution to the estimate $$ \theta $$ (a vector), and even if there are, they may require large matrix manipulations which is not computationally economical (a good example is the canonical logistic regression). So smart computer scientists developed algorithms like Stochastic Gradient Descent (SGD) to bring $$ \theta $$ to as close as possible to the (often times non-existent) closed-form solution by minimizing the loss function $$ f(\theta) $$ (also a matrix). 

In each iteration, $$ \theta $$ is updated by doing: $$ \theta_{n+1} = \theta_n - \eta\nabla f(\theta_n)$$, where $$\nabla f(\theta_n)$$ is computationally calculating the loss function's derivative with respect to $$\theta$$ at $$\theta_n$$, which is also referred to as the gradient. And $$\eta$$ is the learning rate. This is the vanila SGD. The idea behind this is that the loss $$f(\theta)$$ is a convex function, and we can minimize it by traversing along the opposite direction of the gradient $$\nabla$$ at each step $$\theta_n$$. But why gradient? Or Why first derivative? Now if we think about a function $$f(x)$$, it is but a plane in a n-dimensional space. To simplify the problem, let's say $$x$$ is one dimensional, you know, just $$x$$, then $$f(x)$$ is a line in our good old $$x-y$$ plane. At any given point $$x_n$$, the speed of the point traveling in either direction along the line is the first derivative of $$f(x) $$ valued at $$x_n$$, noted as $$f'(x_n)$$. And in machine learning case, it's called gradient.   

SGD represented in python (from CS231N notes) is :

```python
# Vanila SGD
x += - learning_rate * dx
```
where ```x``` is the model estimate $$\theta_{n+1}$$, ```dx``` is the gradient $$\nabla f(\theta_n)$$, and ```learning_rate``` is the learning rate $$\eta$$. ```x``` and ```dx``` are both numpy arrays used to represent matrices. 

The vanilla SGD has its problems. One of which is the learning rate $$ \eta $$, which is normally set at 0.1. In cases where $$\theta_n$$ is relatively close to the optimal $$\theta$$ value, the term $$\eta\nabla f(\theta_n)$$ may result in an update that brings $$\theta_{n+1}$$ further from it. [Adaptive Gradient (Adagrad)](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) is invented to resolve this issue. Instead of using a fixed $$\eta$$, we use an $$\eta$$ that adapts to the magnitudes of $$\nabla f(\theta_n)$$. In python (and I am copying from CS231N), it is:

```python
# Assume the gradient dx and parameter vector x
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```
where ```eps``` is a very small value, added to prevent division by 0. 
In Adagrad, we can see that the learning rate $$\eta$$ is normalized by dividing it by the magnitudes of the gradients $$\nabla f(\theta_n)$$. Note also that the divisor term ```cache``` is also normalized by summing all the square terms of the gradients since iteration 1. This makes ```cache``` a running average of the magnitudes of the gradients at each step. A genious idea! 

Now Adagrad takes care of cases when the update term $$\eta\nabla f(\theta_n)$$ is too big which draws $$\theta$$ away from the optimal value, what about when the update term is too small and $$\theta$$ never gets there? This brings us to RMSProp. 

Note that in Adagrad, the adjusted learning rate ```learning_rate / (np.sqrt(cache) + eps)``` becomes smaller step after step (because ```cache``` is always increasing), which can cause the loss function to be stuck. Going along this line of thoughts, here is [RMSProp](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf): 

```python
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```
Where ```decay_rate``` is a value very close to 1, say 0.9 or 0.99. By simple math manipulation we can prove that at each step, the ```cache``` will be bigger than the ```cache``` of the previous step if ```dx**2``` is bigger than ```cache```, and smaller otherwise. So this enables the learning rate $$\eta$$ to be updated accoding to the magnitudes of the gradient vector, at the same time preventing the update term to be monotonically decreasing at each step. 

Can it get any better than this? Now we have resolved the issue of learning rate $$\eta$$ being either too big or too small for the occasion, we will look at the other term in the update, the gradient. Here is Adam (I changed the notation from CS231N to follow along RMSProp):

```python
# t is your iteration counter going from 1 to infinity
m = beta1*m + (1-beta1)*dx
m_t = m / (1-beta1**t)
cache = beta2*cache + (1-beta2)*(dx**2)
cache_t = cache / (1-beta2**t)
x += - learning_rate * m_t / (np.sqrt(cache_t) + eps)
```
where beta1 is normally 0.9 and beta2 is normally 0.999. 

This definitely looks like a killer optimizer simply for the number of lines of code. Let's tackle it one by one, quickly. The last line looks a lot like RMSProp, which changed ```dx``` to ```m_t``` and ```cache``` to ```cache_t```. We will get to the ```_t```'s in a moment. What is ```m```? From its equation, it looks like it is an accumulator of all the ```dx``` since step 1. But it only accumulates ```0.1*dx``` every time. It can be understood as momentum, or in my own words, accumulated speed. It can be shown that the new momentum will be greater than the momentum of the previous step if gradient ```dx``` is greater than the momentum of the previous step. So only when ```dx``` is big enough, ```m``` will change direction. This is better than directly using ```dx``` because each step is taking a small random sample to calculate the gradient, and it can be noisy. So if we accumulate the effects of the gradients of all the past steps, we reduce the randomness of gradient calculated from a single step. Now we will get to the ```_t```'s. Because both ```m``` and ```cache``` are accumulated sums that have the initial values set to 0, we will experience a slow start. In order to make them bigger at initial steps, ```m_t``` and ```cache_t``` are introduced. 

In summary, Adam is superior to RMSProp in two aspects. The first is that momentum is used instead of raw gradient. And the second is that the adjustment term ```cache``` to learning rate $$\eta$$ has been given a warm start. In the next post, I will give a deep learning example of Adam outperforming RMSProp. 
